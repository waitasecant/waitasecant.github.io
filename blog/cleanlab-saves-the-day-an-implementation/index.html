<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Cleanlab Saves The Day: An Implementation</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="/styles.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap">
  <link rel="icon" href="https://img.icons8.com/office80/32w/beaver.png" type="image/png">
  <script  src="/js/blog.js"></script>
  <script src="/js/blog-post.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/polyfill/3.111.0/polyfill.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/stackoverflow-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
<div class="container">
  <div class="main">
    <span class="left"><h1><a href="/" style="color: black; text-decoration: none;">Himanshu</a></h1></span>
    <span class="right">
      <h1>
        <a href="mailto:waitasecant@gmail.com" class="social" style="margin-right: 24px;"><i class="far fa-envelope"></i></a><a href="https://www.strava.com/athletes/waitasecant" target="_blank" class="social" style="margin-right: 24px;"><i class="fab fa-strava"></i></a><a href="https://x.com/intent/follow?screen_name=waitasecant" target="_blank" class="social" style="margin-right: 24px;"><i class="fab fa-x-twitter"></i></a><a href="https://github.com/waitasecant/" target="_blank" class="social" style="margin-right: 24px;"><i class="fab fa-github"></i></a><a href="https://www.linkedin.com/in/waitasecant" target="_blank" class="social"><i class="fab fa-linkedin"></i></a>
      </h1>
    </span>
  </div>

  <h2 class="blog-title">Cleanlab Saves The Day: An Implementation</h2>
  <p class="blog-meta">May, 2025</p>

  <div class="blog-tags">
    <span class="blog-tag">cleanlab</span>
    <span class="blog-tag">data-quality</span>
  </div>

  <details >
    <summary accesskey="c" title="(Alt + C)">
      <span class="details">Table of Contents</span>
    </summary>

    <div class="inner"><ul>
      <li>
          <a href="#motivation">Motivation</a>
      </li>
      <li>
          <a href="#the-dataset">The Dataset</a>
      </li>
      <li>
          <a href="#vanilla-implementation">Vanilla Implementation</a>
      </li>
      <li>
          <a href="#cleanlab">Cleanlab</a><ul>
              
      <li>
          <a href="#introduction">Introduction</a></li>
      <li>
          <a href="#analysis-of-report">Analysis of Report</a></li>
      <li>
          <a href="#hypothesis">Hypothesis</a></li></ul>
      </li>

      <li>
          <a href="#re-implementation">Re-Implementation</a>
      </li>
      <li>
          <a href="#results">Results</a>
      </li>
      <li>
          <a href="#conclusion">Conclusion</a>
      </li>
    </div>
  </details>

  <div class="blog-content">
    <p>'Garbage In, Garbage Out' a popular adage which says the quality of the output is only as good as the quality of the input i.e. your model is as good as the data it is trained upon. In this blog, we demonstrate this concept using <a href="https://cleanlab.ai/" target="_blank">Cleanlab</a>, in particular, <a href="https://studio.cleanlab.ai/" target="_blank">Cleanlab Studio</a>.</p>

    <p id="motivation" class="head1">Motivation</p>

    <p>To see is to believe, and we shall see through experimentation, the importance of data quality in data science. TLDR, we train a model for multi task multi class classification and find one of the task performing poorly. We suspect data labels to be ambiguous. We shall try fitting the model again using the clean labels generated by <a href="https://studio.cleanlab.ai/" target="_blank">Cleanlab Studio</a> and compare the performance. </p>
    
    <p id="the-dataset" class="head1">The Dataset</p>
    <p>For the demonstration, we use <a href="https://archive.ics.uci.edu/dataset/124/cmu+face+images" target="_blank">CMU Face Images</a> dataset.
      <ul>
        <li>This data consists of 640 black and white face images of people.</li>
        <li>Each image can be characterized by the pose [straight, left, right, up], expression [neutral, happy, sad, angry], eyes [wearing sunglasses or not].</li>
        <li>This directory contains 20 subdirectories, one for each person, named by <code>userid</code>.</li>
        <li>There are 32 images for each person capturing every combination of features. 16 of the 640 images have glitches due to problems with the camera setup; these are the .bad images</li>
        <li>Naming convention of images <code>[userid][pose][expression][eyes][scale].pgm</code></li>
        <li>Images with scale 1 were selected i.e. full-resolution image (120x128) which has the convention <code>[userid][pose][expression][eyes].pgm</code></li>
      </ul>
    </p>
    <figure>
      <img src="img/data-sample.png" alt="CMU Face Images">
      <figcaption>Sample images from the dataset <code>[pose; expression; eyes]</code></figcaption>
    </figure>

    <p id="vanilla-implementation" class="head1">Vanilla Implementation</p>

    <p>We create a custom dataset leveraging the PyTorch dataset API for easy data loading. We leverage the naming convention to extract the labels.</p>

    <div class="code-container">
    <pre><code class="language-python"># Code for custom dataset using PyTorch
class CMUFaceDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform

        # User IDs in the dataset
        self.user_ids = ['an2i', 'at33', 'boland', 'bpm', 'ch4f', 'cheyer', 'choon',
                   'danieln', 'glickman', 'karyadi', 'kawamura', 'kk49', 'megak',
                   'mitchell', 'night', 'phoebe', 'saavik', 'steffi', 'sz24', 'tammo']

        # Create user_id to index mapping for faster lookups
        self.user_to_idx = {user_id: idx for idx, user_id in enumerate(self.user_ids)}

        # Scan the dataset directory structure
        self._scan_dataset()

    def _scan_dataset(self):
        """Scan the dataset directory structure"""
        # Use glob to find all PGM files (more efficient than nested loops)
        pattern = os.path.join(self.data_dir, "*/*.pgm")
        all_image_paths = glob.glob(pattern)

        exp_size = len(all_image_paths)

        # Initialize data structures
        self.image_paths = [None] * exp_size
        self.pose = [None] * exp_size
        self.expression = [None] * exp_size
        self.eyes = [None] * exp_size
        self.images = [None] * exp_size

        # Process files sequentially to avoid thread safety issues
        valid_idx = 0
        for img_path in all_image_paths:
            # Extract user_id from path
            parts = os.path.normpath(img_path).split(os.sep)
            user_id = parts[-2]

            if user_id in self.user_to_idx:
                # Parse filename to extract metadata
                filename = os.path.basename(img_path)
                parts = filename.split('_')

                if len(parts) >= 4:
                    # Store metadata and path
                    self.image_paths[valid_idx] = img_path
                    self.pose[valid_idx] = parts[1]
                    self.expression[valid_idx] = parts[2]
                    self.eyes[valid_idx] = parts[3][:-4]

                    image = Image.open(img_path).convert('L')
                    if self.transform:
                        image = self.transform(image)
                    self.images[valid_idx] = image
                    valid_idx += 1

        # Trim lists to the valid size
        if valid_idx < exp_size:
          self.image_paths = self.image_paths[:valid_idx]
          self.pose = self.pose[:valid_idx]
          self.expression = self.expression[:valid_idx]
          self.eyes = self.eyes[:valid_idx]
          self.images = self.images[:valid_idx]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        return self.images[idx], (self.pose[idx], self.expression[idx], self.eyes[idx])</code></pre>
    <button class="copy-button" aria-label="Copy code">
      <svg class="copy-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
      </svg>
      <svg class="success-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <path d="M20 6L9 17l-5-5"></path>
      </svg>
    </button>
    </div>


    <p>We shall implement a fairly simple CNN-based architecture for the classification task. We train the model for 20 epochs with 80:20 train-test split.</p>
    <div class="code-container">
    <pre><code class="language-python"># Code for multitask face classifier using PyTorch
class MultitaskFaceClassifier(nn.Module):
    def __init__(self, num_poses=4, num_expressions=2, num_eye_states=2):
        super(MultitaskFaceClassifier, self).__init__()

        # Enhanced feature extraction with batch normalization
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool = nn.MaxPool2d(2, 2)

        # Shared features with batch norm
        self.fc_shared = nn.Linear(128 * 8 * 8, 512)
        self.bn_shared = nn.BatchNorm1d(512)
        self.dropout_shared = nn.Dropout(0.5)

        # Enhanced task-specific heads with intermediate layers
        # Pose head
        self.pose_layers = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_poses)
        )

        # Expression head
        self.expression_layers = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.6),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_expressions)
        )

        # Eyes head
        self.eyes_layers = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_eye_states)
        )

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = x.view(-1, 128 * 8 * 8)

        # Shared features with normalization
        features = self.fc_shared(x)
        features = self.bn_shared(features)
        features = F.relu(features)
        features = self.dropout_shared(features)

        # Task-specific outputs through enhanced heads
        pose_out = self.pose_layers(features)
        expression_out = self.expression_layers(features)
        eyes_out = self.eyes_layers(features)

        return {
            'pose': pose_out,
            'expression': expression_out,
            'eyes': eyes_out
        }, features</code></pre>
    <button class="copy-button" aria-label="Copy code">
      <svg class="copy-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
      </svg>
      <svg class="success-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <path d="M20 6L9 17l-5-5"></path>
      </svg>
    </button>
    </div>

    <p>Following are the plots for epoch vs loss, confusion matrix and prediction sample. We consider accuracy as the evaluation metric.</p>

    <figure>
      <img src="img/loss-cnn.png" alt="Epoch vs Loss curve for CNN Classifier">
      <figcaption>Epoch vs Loss curve for CNN Classifier</figcaption>
    </figure>

    <p>Note: The loss curve for <code>expression</code> does not look right.
      <ul>
        <li>The validation loss does not follow decreasing pattern, in fact, a slight increase. This raises a concern.</li>
        <li>The loss is ~ 1.42 after 10 epochs, which is as good as random guess. (Why?)</li>
      </ul>
    </p>

    <details class="math-content">
      <summary accesskey="c" title="(Alt + C)">
        <span class="details">Mathematical Reasoning</span>
      </summary>

      <p>For distribution a classification model with $K$ classes that performs as well as a random guess, the expected cross-entropy loss is $\text{log}(K)$.</p>
      <p>The cross-entropy loss for a single prediction is defined as,</p>
      <p>$$H(y, \hat{y}) = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$$</p>
      <p>where,</p>
      <p>$y_i$ is the true probability for class $i$ ($y_i = 1$ for the correct class and $y_i = 0$ for all other classes)</p>
      <p>$\hat{y}_i$ is the predicted probability for class $i$</p>
      <p>For a random classifier with $K$ classes, the model assigns equal probability to each class, $\hat{y}_i = \frac{1}{K}$ $\forall$ classes.</p>
      <p>When the true class is class $j$, we have $y_j = 1$ and $y_i = 0$ for all $i \neq j$. Therefore,</p>
      <p>$$H(y, \hat{y}) = -\sum_{i=1}^{K} y_i \log(\hat{y}_i) = -y_j \log(\hat{y}_j) = -1 \cdot \log\left(\frac{1}{K}\right)= \log(K)$$</p>
      <p>Since this calculation holds for any true class $j$, and assuming a balanced dataset where each class appears with equal frequency $\frac{1}{K}$, the expected cross-entropy loss is given by,</p>
      <p>$$\mathbb{E}[H(y, \hat{y})] = \sum_{j=1}^{K} P(\text{true class} = j) \cdot H(y_j, \hat{y}) = \sum_{j=1}^{K} \frac{1}{K} \cdot \log(K) = \log(K)$$</p>
      <p>Thus, the expected cross-entropy loss for a random classifier with $4$ classes is $\mathbb{E}[H] = \ln(4) \approx 1.386$</p>
      <p>This result represents the baseline performance that any classification model should exceed. A model performing worse than this baseline is actually performing worse than random guessing.</p>
    </details>

    <figure>
      <img src="img/cm-cnn.png" alt="Confusion Matrix for CNN Classifier">
      <figcaption>Confusion Matrix for CNN Classifier</figcaption>
    </figure>

    <p>The confusion matrix shows that <code>pose</code> and <code>eyes</code> achieve 97.6%, which is good. As expected, <code>expression</code> achieve 16.8%, which shall be our focus going forward.</p>

    <figure>
      <img src="img/pred-sample-cnn.png" alt="Prediction sample for CNN Classifier">
      <figcaption>Prediction sample for CNN Classifier</figcaption>
    </figure>

    <p>The labels in <span style="color: green;">green</span> indicate correct prediction while <span style="color: red;">red</span> indicates wrong prediction.</p>

    <p id="cleanlab" class="head1">Cleanlab</p>
    <p id="introduction" class="head2">Introduction</p>
    <p><a href="https://studio.cleanlab.ai/" target="_blank">Cleanlab Studio</a> is an AI-powered data curation tool used to improve the quality of data and resulting model/analytics. The Studio offers three workflows, Web interface, Python API, and Command Line. We used the Web Interface for this demonstration.</p>

    <p>The usage pretty straight forward
      <ul>
        <li>Upload a dataset</li>
        <li>Create a project (their AI analyzes the data)</li>
        <li>Review detected data issues</li>
        <li>Export a cleanset (the cleaned dataset)</li>
      </ul>
    </p>

    <p>The Studio does not support our classification task i.e. Multi-Task Multi-Class Classification. But since we would only treat the <code>expression</code> task, so Multi-Class Classification suffices for the demonstration.</p>

    <p id="analysis-of-report" class="head2">Analysis of Report</p>
    <p>We download the cleanset and use it to create new labels for <code>expression</code> task. Bur before that, lets look at ready-made analytics provided by the Studio.</p>

    <figure>
      <img src="img/cleanlab-suggested-matrix.png" alt="Clean Lab Suggested Labels">
      <figcaption>Clean Lab Suggested Labels</figcaption>
    </figure>

    <figure>
      <img src="img/cleanlab-issue-count.png" alt="Clean Lab Issue Count by Class">
      <figcaption>Clean Lab Issue Count by Class</figcaption>
    </figure>

    <p>Easy to see, there is a high correlation(inverse) between percent of label issues(label issue, outlier, ambiguous) combined per class and the accuracy per class.</p>

    <figure>
      <img src="img/acc-vs-labels.png" alt="Accuracy vs Label Issues (in %)">
      <figcaption>Accuracy vs Label Issues (in %)</figcaption>
    </figure>

    <p>Now, we use the cleanset to create new labels by following the steps below.
      <ul>
        <li>Create two copies of the <code>cleanlab_suggested_label</code> column, i.e. <code>cleanlab_suggested_label_original</code> and <code>cleanlab_suggested_label_pred</code> which essentially contain the suggestions made by Cleanlab.</li>
        <li>The column <code>cleanlab_suggested_label</code> has blank cells wherever [<i>Condition 1</i>] <code>cleanlab_action</code> is not <code>unresolved</code> (resolved) OR [<i>Condition 2</i>] atleast one of the columns <code>cleanlab_is_outlier</code> or <code>cleanlab_is_ambiguous</code> is <code>TRUE</code> with <code>cleanlab_is_label_issue</code> value <code>False</code>.</li>
        <li>Impute the original label for <code>expression</code> where [<i>Condition 1</i>] is TRUE for both copies (Why?)</li>
        <li>Impute the original label and <code>cleanlab_predicted_label</code> to <code>cleanlab_suggested_label_original</code> and <code>cleanlab_suggested_label_pred</code> where [<i>Condition 2</i>] is TRUE.</li>
      </ul>
    </p>
    <div class="code-container">
    <pre><code class="language-python"># Make labels based on cleanset exported from Cleanlab Studio
import pandas as pd
df = pd.read_csv('cleanlab-faces-expr.csv')

# Make the copies of the suggested label column
df['cleanlab_suggested_label_original'] = df['cleanlab_suggested_label']
df['cleanlab_suggested_label_pred'] = df['cleanlab_suggested_label']

# [Condition 1]
# Replace values in 'cleanlab_suggested_label_original' and 'cleanlab_suggested_label_pred' 
# where 'cleanlab_action' is not 'unresolved'
mask = df['cleanlab_action'] != 'unresolved'
df.loc[mask, 'cleanlab_suggested_label_original'] = df.loc[mask, 'expression']
df.loc[mask, 'cleanlab_suggested_label_pred'] = df.loc[mask, 'expression']

# [Condition 2]
# Replace values based on the specified conditions
not_out_amb = ~((df['cleanlab_is_outlier'] == False) & (df['cleanlab_is_ambiguous'] == False))
no_label_issue = df['cleanlab_is_label_issue'] == False
mask = not_out_amb & no_label_issue

df.loc[mask, 'cleanlab_suggested_label_original'] = df.loc[mask, 'expression']
df.loc[mask, 'cleanlab_suggested_label_pred'] = df.loc[mask, 'cleanlab_predicted_label']

# Save the modified dataframe back to CSV
df.to_csv('modified-cleanlab-faces-expr.csv', index=False)</code></pre>
    <button class="copy-button" aria-label="Copy code">
      <svg class="copy-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
      </svg>
      <svg class="success-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <path d="M20 6L9 17l-5-5"></path>
      </svg>
    </button>
    </div>

    <p id="hypothesis" class="head2">Hypothesis</p>
    <p>Our hypothesis is that, the labels for <code>expression</code> are too ambiguous. The way this data was created, is given the tuple (<code>pose</code>, <code>expression</code>, <code>eyes</code>), the person captured the image with the said <code>expression</code> with <code>pose</code> and <code>eyes</code>. Now its natural to suspect that if <code>pose</code> is not straight or <code>eyes</code> is sunglasses, the <code>expression</code> might not be captured faithfully. There is clear directed dependence of <code>expression</code> on <code>pose</code> and <code>eyes</code> (not the other way around).</p>

    <p id="re-implementation" class="head1">Re-Implementation</p>
    <p>We use the new set of labels to fit our model again and compare the results. We create another custom dataset for new labels. This dataset can be used for the base case as well by not passing any data frame. This makes the previous one redundant but I still did it for the sake of demonstration.</p>

    <div class="code-container">
    <pre><code class="language-python"># Code for custom dataset (with labels using Cleanlab) using PyTorch
class CMUFaceDatasetCleanLab(Dataset):
    def __init__(self, data_dir, transform=None, df=None, col='expression'):
        self.data_dir = data_dir
        self.transform = transform
        self.df = df
        
        # Create a mapping from image filename to new label if df is provided
        self.expression_mapping = {}
        if df is not None:
            # Convert dataframe to a dictionary for faster lookups
            self.expression_mapping = dict(zip(df['image'], df[col]))

        # User IDs in the dataset
        self.user_ids = ['an2i', 'at33', 'boland', 'bpm', 'ch4f', 'cheyer', 'choon',
                   'danieln', 'glickman', 'karyadi', 'kawamura', 'kk49', 'megak',
                   'mitchell', 'night', 'phoebe', 'saavik', 'steffi', 'sz24', 'tammo']

        # Create user_id to index mapping for faster lookups
        self.user_to_idx = {user_id: idx for idx, user_id in enumerate(self.user_ids)}

        # Scan the dataset directory structure
        self._scan_dataset()

    def _scan_dataset(self):
        """Scan the dataset directory structure"""
        # Use glob to find all PGM files (more efficient than nested loops)
        pattern = os.path.join(self.data_dir, "*/*.pgm")
        all_image_paths = glob.glob(pattern)

        exp_size = len(all_image_paths)
        
        # Initialize data structures
        self.image_paths = [None] * exp_size
        self.pose = [None] * exp_size
        self.expression = [None] * exp_size
        self.eyes = [None] * exp_size
        self.images = [None] * exp_size

        # Process files sequentially to avoid thread safety issues
        valid_idx = 0
        for img_path in all_image_paths:

            parts = os.path.normpath(img_path).split(os.sep)
            user_id = parts[-2]
            filename = os.path.basename(img_path)

            if user_id in self.user_to_idx:
                # Parse filename to extract metadata
                parts = filename.split('_')

                if len(parts) >= 4:
                    
                    # Use the mapped expression if available, otherwise use the original
                    if filename in self.expression_mapping:
                        expression = self.expression_mapping[filename]
                    else:
                        expression = parts[2]
                        
                    # Store metadata and path
                    self.image_paths[valid_idx] = img_path
                    self.pose[valid_idx] = parts[1]
                    self.expression[valid_idx] = expression
                    self.eyes[valid_idx] = parts[3][:-4]

                    image = Image.open(img_path).convert('L')
                    if self.transform:
                        image = self.transform(image)
                    self.images[valid_idx] = image
                    valid_idx += 1

        # Trim lists to the valid size
        if valid_idx < exp_size:
          self.image_paths = self.image_paths[:valid_idx]
          self.pose = self.pose[:valid_idx]
          self.expression = self.expression[:valid_idx]
          self.eyes = self.eyes[:valid_idx]
          self.images = self.images[:valid_idx]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        return self.images[idx], (self.pose[idx], self.expression[idx], self.eyes[idx])</code></pre>
    <button class="copy-button" aria-label="Copy code">
      <svg class="copy-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
      </svg>
      <svg class="success-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
        <path d="M20 6L9 17l-5-5"></path>
      </svg>
    </button>
    </div>

    <p>Following are the plots for epoch vs loss, confusion matrix and prediction sample.</p>

    <figure>
      <img src="img/loss-cnn-cleanlab-original.png" alt="Epoch vs Loss curve for CNN Classifier with Cleanlab Original Label">
      <figcaption>Epoch vs Loss curve for CNN Classifier with Cleanlab Original Label</figcaption>
    </figure>

    <figure>
      <img src="img/loss-cnn-cleanlab-pred.png" alt="Epoch vs Loss curve for CNN Classifier with Cleanlab Prediction Label">
      <figcaption>Epoch vs Loss curve for CNN Classifier with Cleanlab Prediction Label</figcaption>
    </figure>

    <p>Note: The loss curve for <code>expression</code> looks much better now for both the cases. The validation loss follows decreasing pattern, in fact, a slight decrease. This is a good sign. Also, the loss is ~ 1.25(&lt;1.386) after 10 epochs, which is an improvement.</p>

    <figure>
      <img src="img/cm-cnn-cleanlab-original.png" alt="Confusion Matrix for CNN Classifier with Cleanlab Original Label">
      <figcaption>Confusion Matrix for CNN Classifier with Cleanlab Original Label</figcaption>
    </figure>

    <figure>
      <img src="img/cm-cnn-cleanlab-pred.png" alt="Confusion Matrix for CNN Classifier with Cleanlab Prediction Label">
      <figcaption>Confusion Matrix for CNN Classifier with Cleanlab Prediction Label</figcaption>
    </figure>

    <p>The following can be concluded from the confusion matrix
      <ul>
        <li>The accuracy for <code>pose</code> has dipped a little for both cases.</li>
        <li>The accuracy for <code>eyes</code> has remained almost unchanged for both cases.</li>
        <li>The accuracy for <code>expression</code> has improved by 160% from 16.8% to ~ 44% for both cases.</li>
      </ul>
    </p>
    <p>Note: The distribution of <code>expression</code> classes has changed now with <code>neutral</code> class being over-represented. (Why?)</p>

    <figure>
      <img src="img/pred-sample-cnn-cleanlab-original.png" alt="Prediction sample for CNN Classifier with Cleanlab Original Label">
      <figcaption>Prediction sample for CNN Classifier with Cleanlab Original Label</figcaption>
    </figure>

    <figure>
      <img src="img/pred-sample-cnn-cleanlab-pred.png" alt="Prediction sample for CNN Classifier with Cleanlab Prediction Label">
      <figcaption>Prediction sample for CNN Classifier with Cleanlab Prediction Label</figcaption>
    </figure>

    <p id="results" class="head1">Results</p>

    <div class="table-container">
      <table class="data-table">
        <thead>
            <tr>
              <th>Label</th>
              <th>pose</th>
              <th>expression</th>
              <th>eyes</th>
            </tr>
        </thead>
        <tbody>
          <tr>
            <td>Original</td>
            <td>97.6%</td>
            <td>16.8%</td>
            <td>97.6%</td>
          </tr>
          <tr>
            <td>Cleanlab Original</td>
            <td>93.6%</td>
            <td>44.0%</td>
            <td>98.4%</td>
          </tr>
          <tr>
            <td>Cleanlab Prediction</td>
            <td>94.4%</td>
            <td>44.8%</td>
            <td>97.6%</td>
          </tr>
        </tbody>
      </table>
      <div class="table-caption">
          Table: Accuracy of the model for different labels
      </div>
    </div>

    <p id="conclusion" class="head1">Conclusion</p>
    <p>Badly labelled data will take you only so far.</p>
  </div>

</div>

  <footer>
    <a href="/open-source" style="margin: 0px 20px 0px 20px; color: dimgrey; text-decoration: none;" target="_blank">Open Source</a><a href="/blog" style="margin: 0px 20px 0px 20px; color: dimgrey; text-decoration: none;" target="_blank">Blog</a><a href="/running" style="margin: 0px 20px 0px 20px; color: dimgrey; text-decoration: none;" target="_blank">Running</a>
  </footer>

</body>
</html>